import argparse
import os
import sys
import time

from mls.manager.job.utils import training_job_api_from_profile


def get_in_progress_jobs(client, region, statuses=None):
    all_in_progress_jobs = []
    if statuses is None:
        statuses = ["Pending", "Running"]

    for non_final_status in statuses:
        while True:
            non_final_jobs = client.get_list_jobs(
                region=region,
                allocation_name="alloc-officecds-multimodal-2-sr004",
                status=non_final_status,
                limit=1000,
                offset=0,
            )
            if "jobs" in non_final_jobs:
                break
            if "error_code" in non_final_jobs and non_final_jobs["error_code"] == [32, 20]:
                print("Error:", non_final_jobs, "try again")
                time.sleep(5)
                client, _ = training_job_api_from_profile("default")
                continue
            raise ValueError("Unknown error in get_in_progress_jobs:", non_final_jobs)

        all_in_progress_jobs.extend(non_final_jobs["jobs"])

    return all_in_progress_jobs


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Launch compression head training jobs.")
    parser.add_argument("--dry", action="store_true", help="Only print generated scripts, do not launch jobs.")
    parser.add_argument(
        "--num_gpus",
        type=int,
        default=1,
        help="Number of GPUs to request for the job (affects instance_type). Default: 1",
    )
    parser.add_argument(
        "--model",
        nargs="+",
        default=None,
        help="Filter models by name (substring match). Can specify multiple models.",
    )
    parser.add_argument(
        "--model_checkpoint",
        default=None,
        help="Explicit model checkpoint",
    )
    parser.add_argument(
        "--model_name",
        default=None,
        help="Explicit model checkpoint",
    )
    parser.add_argument(
        "--dtype",
        default=None,
        help="Torch dtype to use: auto | float32/fp32 | bfloat16/bf16 | float16/fp16. Default: bf16",
    )
    parser.add_argument(
        "--limit_dataset_items",
        type=int,
        default=None,
        help="Limit the number of dataset items to use.",
    )
    parser.add_argument(
        "--dataset_name",
        type=str,
        default=None,
        help="Dataset name to use for training (e.g., 'mrsndmn/pg19', 'HuggingFaceFW/fineweb-edu').",
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=None,
        help="Learning rate for optimization. Default: 1e-4",
    )
    parser.add_argument(
        "--random_seed",
        type=int,
        default=None,
        help="Random seed for reproducibility. Default: 42",
    )
    parser.add_argument(
        "--lr_scheduler_type",
        type=str,
        default=None,
        help="Learning rate scheduler type. Default: cosine_warmup_with_min_lr",
    )
    parser.add_argument(
        "--lr_scheduler_kwargs",
        type=str,
        default=None,
        help="Learning rate scheduler kwargs as JSON string (e.g., '{\"min_lr\":1e-3}').",
    )
    parser.add_argument(
        "--max_sequence_length",
        type=int,
        default=None,
        help="Maximum sequence length. Default: 1024",
    )
    parser.add_argument(
        "--per_device_train_batch_size",
        type=int,
        default=None,
        help="Batch size per device. Default: 4",
    )
    parser.add_argument(
        "--total_batch_size",
        type=int,
        default=None,
        help="Total (global) train batch size across all GPUs and gradient accumulation. Default: 128",
    )
    parser.add_argument(
        "--dataloader_num_workers",
        type=int,
        default=None,
        help="Number of dataloader workers. Default: 4",
    )
    parser.add_argument(
        "--max_steps",
        type=int,
        default=None,
        help="Maximum training steps. If not specified, uses num_train_epochs.",
    )
    parser.add_argument(
        "--num_train_epochs",
        type=int,
        default=None,
        help="Number of training epochs. Default: 1",
    )
    parser.add_argument(
        "--compression_head_distill_alpha",
        type=float,
        default=None,
        help="Weight for distillation loss. Default: 1.0",
    )
    parser.add_argument(
        "--compression_head_freeze_base_model",
        type=lambda x: x.lower() in ("true", "1", "yes"),
        default=None,
        help="Freeze base model (default: True). Pass 'false' to unfreeze.",
    )
    parser.add_argument(
        "--weight_decay",
        type=float,
        default=None,
        help="Weight decay. Default: 0.01",
    )
    parser.add_argument(
        "--max_grad_norm",
        type=float,
        default=None,
        help="Max gradient norm. Default: 1.0",
    )
    parser.add_argument(
        "--warmup_steps",
        type=int,
        default=None,
        help="Number of warmup steps. Default: 0",
    )
    parser.add_argument(
        "--logging_steps",
        type=int,
        default=None,
        help="Logging steps. Default: 50",
    )

    args = parser.parse_args()
    workdir = os.getcwd()
    python_path = "/workspace-SR004.nfs2/d.tarasov/envs/compression_horizon/bin/python"

    client, extra_options = training_job_api_from_profile("default")

    author_name = "d.tarasov"
    region = extra_options["region"]
    in_progress_jobs = get_in_progress_jobs(client, region)
    in_progress_job_descs = {job.get("job_desc", "") for job in in_progress_jobs}

    checkpoints = [
        "HuggingFaceTB/SmolLM2-1.7B",
        "unsloth/Llama-3.2-3B",
        "Qwen/Qwen3-4B",
        "unsloth/Meta-Llama-3.1-8B",
        "Qwen/Qwen3-8B",
        "allenai/OLMo-1B-hf",
        "allenai/Olmo-3-1025-7B",
        "google/gemma-3-4b-pt",
        "google/gemma-3-1b-pt",
    ]

    if args.model:
        model_filters = [m.lower() for m in args.model]
        filtered_checkpoints = []
        for checkpoint in checkpoints:
            checkpoint_lower = checkpoint.lower()
            model_name = checkpoint.split("/")[-1].lower() if "/" in checkpoint else checkpoint_lower
            if any(filt in checkpoint_lower or filt in model_name for filt in model_filters):
                filtered_checkpoints.append(checkpoint)
        checkpoints = filtered_checkpoints
        if not checkpoints:
            print(f"\033[33mNo models matched the filter: {args.model}\033[0m")
            sys.exit(0)
    elif args.model_checkpoint:
        checkpoints = [args.model_checkpoint]

    for model_checkpoint in checkpoints:
        model_name = model_checkpoint.split("/")[-1]
        if model_name == "":
            model_name = args.model_name
        exp_suffix = f"ch_head_{model_name}"

        # Default values
        limit_dataset_items = args.limit_dataset_items if args.limit_dataset_items is not None else 50000
        dataset_name = args.dataset_name if args.dataset_name is not None else "HuggingFaceFW/fineweb-edu"
        learning_rate = args.learning_rate if args.learning_rate is not None else 1e-4
        max_sequence_length = args.max_sequence_length if args.max_sequence_length is not None else 1024
        per_device_train_batch_size = args.per_device_train_batch_size if args.per_device_train_batch_size is not None else 4
        num_gpus = args.num_gpus if args.num_gpus is not None else 1
        if num_gpus < 1:
            raise ValueError(f"--num_gpus must be >= 1, got {num_gpus}")

        total_batch_size = args.total_batch_size if args.total_batch_size is not None else 128
        denom = num_gpus * per_device_train_batch_size
        if denom <= 0:
            raise ValueError(
                f"Invalid batch sizing: num_gpus={num_gpus}, per_device_train_batch_size={per_device_train_batch_size}"
            )
        if total_batch_size % denom != 0:
            raise ValueError(
                "total_batch_size must be divisible by (num_gpus * per_device_train_batch_size). "
                f"Got total_batch_size={total_batch_size}, num_gpus={num_gpus}, per_device_train_batch_size={per_device_train_batch_size}"
            )
        gradient_accumulation_steps = total_batch_size // denom
        if gradient_accumulation_steps < 1:
            raise ValueError(
                "Computed gradient_accumulation_steps < 1. "
                f"Got total_batch_size={total_batch_size}, num_gpus={num_gpus}, per_device_train_batch_size={per_device_train_batch_size}"
            )
        dataloader_num_workers = args.dataloader_num_workers if args.dataloader_num_workers is not None else 4
        compression_head_distill_alpha = (
            args.compression_head_distill_alpha if args.compression_head_distill_alpha is not None else 1.0
        )
        compression_head_freeze_base_model = (
            args.compression_head_freeze_base_model if args.compression_head_freeze_base_model is not None else True
        )
        dtype = args.dtype if args.dtype is not None else "bf16"
        num_train_epochs = args.num_train_epochs if args.num_train_epochs is not None else 1
        weight_decay = args.weight_decay if args.weight_decay is not None else 0.01
        max_grad_norm = args.max_grad_norm if args.max_grad_norm is not None else 1.0
        warmup_steps = args.warmup_steps if args.warmup_steps is not None else 0
        logging_steps = args.logging_steps if args.logging_steps is not None else 50
        lr_scheduler_type = args.lr_scheduler_type if args.lr_scheduler_type is not None else "cosine_warmup_with_min_lr"

        cmd_args = [
            "--train_compression_head",
            f"--model_checkpoint {model_checkpoint}",
            f"--dataset_name {dataset_name}",
            f"--limit_dataset_items {limit_dataset_items}",
            f"--per_device_train_batch_size {per_device_train_batch_size}",
            f"--gradient_accumulation_steps {gradient_accumulation_steps}",
            f"--max_sequence_length {max_sequence_length}",
            f"--learning_rate {learning_rate}",
            f"--compression_head_distill_alpha {compression_head_distill_alpha}",
            f"--dataloader_num_workers {dataloader_num_workers}",
            f"--dtype {dtype}",
            f"--num_train_epochs {num_train_epochs}",
            f"--weight_decay {weight_decay}",
            f"--max_grad_norm {max_grad_norm}",
            f"--warmup_steps {warmup_steps}",
            f"--logging_steps {int(logging_steps)}",
            f"--lr_scheduler_type {lr_scheduler_type}",
        ]

        if not compression_head_freeze_base_model:
            cmd_args.append("--compression_head_freeze_base_model False")
        else:
            exp_suffix = f"{exp_suffix}_freeze_llm"

        if args.max_steps is not None:
            cmd_args.append(f"--max_steps {args.max_steps}")
            exp_suffix = f"{exp_suffix}_steps_{args.max_steps}"
        else:
            exp_suffix = f"{exp_suffix}_epochs_{num_train_epochs}"

        if args.random_seed is not None and args.random_seed != 42:
            cmd_args.append(f"--random_seed {args.random_seed}")
            exp_suffix = f"{exp_suffix}_seed_{args.random_seed}"

        if args.lr_scheduler_kwargs is not None:
            cmd_args.append(f"--lr_scheduler_kwargs '{args.lr_scheduler_kwargs}'")
            exp_suffix = f"{exp_suffix}_schedkw_{args.lr_scheduler_kwargs}"

        # Add to exp_suffix if non-default values
        if args.dataset_name is not None:
            dataset_suffix = args.dataset_name.split("/")[-1] if "/" in args.dataset_name else args.dataset_name
            exp_suffix = f"{exp_suffix}_ds_{dataset_suffix}"

        if args.limit_dataset_items is not None and args.limit_dataset_items != 50000:
            exp_suffix = f"{exp_suffix}_limit_{args.limit_dataset_items}"

        if args.max_sequence_length is not None and args.max_sequence_length != 1024:
            exp_suffix = f"{exp_suffix}_seq_{args.max_sequence_length}"

        if args.per_device_train_batch_size is not None and args.per_device_train_batch_size != 4:
            exp_suffix = f"{exp_suffix}_bs_{args.per_device_train_batch_size}"

        if args.total_batch_size is not None and args.total_batch_size != 128:
            exp_suffix = f"{exp_suffix}_tbs_{args.total_batch_size}"

        if args.num_gpus is not None and args.num_gpus != 1:
            exp_suffix = f"{exp_suffix}_ngpu_{args.num_gpus}"

        if args.learning_rate is not None and args.learning_rate != 1e-4:
            lr_str = str(args.learning_rate).replace(".", "p").replace("-", "m")
            exp_suffix = f"{exp_suffix}_lr_{lr_str}"

        if args.compression_head_distill_alpha is not None and args.compression_head_distill_alpha != 1.0:
            alpha_str = str(args.compression_head_distill_alpha).replace(".", "p")
            exp_suffix = f"{exp_suffix}_distill_{alpha_str}"

        if args.dtype is not None and args.dtype != "bf16":
            exp_suffix = f"{exp_suffix}_dtype_{args.dtype}"

        if args.lr_scheduler_type is not None and args.lr_scheduler_type != "cosine_warmup_with_min_lr":
            exp_suffix = f"{exp_suffix}_sched_{args.lr_scheduler_type}"

        if not compression_head_freeze_base_model:
            exp_suffix = f"{exp_suffix}_unfrozen"

        out_dir_name = f"artifacts/experiments_compression_head/{exp_suffix}"
        logging_dir = f"artifacts/experiments_compression_head/{exp_suffix}/logs"

        if os.path.exists(out_dir_name):
            print("Experiment", out_dir_name, "exists, skip.")
            continue

        cmd_args.append(f"--output_dir {out_dir_name}")
        cmd_args.append(f"--logging_dir {logging_dir}")
        script = f"bash {workdir}/scripts/jobs/multigpu.sh scripts/activation_distillation.py  {' '.join(cmd_args)}"
        job_desc = f"CH: compression_head {exp_suffix} #{author_name} #multimodal #notify_completed @mrsndmn"

        if job_desc in in_progress_job_descs:
            print(f"\033[33mSkipping: job already in queue with description:\033[0m {job_desc}")
            continue

        payload = {
            "script": script,
            "job_desc": job_desc,
            "env_variables": {
                "PYTHONPATH": "./src",
                "HF_HOME": "/workspace-SR004.nfs2/.cache/huggingface",
            },
            "instance_type": f"a100.{num_gpus}gpu",
            "region": extra_options["region"],
            "type": "binary_exp",
            "shm_size_class": "medium",
            "base_image": "cr.ai.cloud.ru/aicloud-base-images/py3.12-torch2.7.0:0.0.41",
            "n_workers": 1,
            "processes_per_worker": 1,
        }

        print(f"\033[32m Would launch with description:\033[0m {job_desc}")
        print(f"\033[90m     Command: {script}\033[0m")
        print(f"\033[90m     Output dir: {out_dir_name}\033[0m")

        if args.dry:
            continue

        result = client.run_job(payload=payload)
        print(out_dir_name, result)
