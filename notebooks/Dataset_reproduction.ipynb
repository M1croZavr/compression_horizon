{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace-SR004.nfs2/t.lashukov/compression_horizon/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7f383",
   "metadata": {},
   "source": [
    "# PG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd3bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ds = datasets.load_dataset(\"mrsndmn/pg19\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe9c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(text: str, min_chunk_size: int) -> str:\n",
    "    # random.seed(144)\n",
    "    # Erase all the spaces\n",
    "    text = \" \".join(text.split())\n",
    "    # Split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) < min_chunk_size:\n",
    "        chunk = \" \".join(sentences)\n",
    "    else:\n",
    "        chunk = \"\"\n",
    "        n_words = 0\n",
    "        # Iterate untill appropriate chunk is found\n",
    "        while not (12000 < n_words < 25000):\n",
    "            max_start = len(sentences) - min_chunk_size\n",
    "            start = random.randint(0, max_start)\n",
    "            end = random.randint(start + min_chunk_size, len(sentences))\n",
    "            chunk = \" \".join(sentences[start:end])\n",
    "            n_words = len(chunk.split())\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e14c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:01<00:00, 16.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page SOME ANIMAL PROPENSITIES. 81 THE PETRIFIED FERN. 83 WATER AND ANIMALS. 84 THE HERRING GULL. 87 \n",
      "Number of words in the sample: 12043\n",
      "----------\n",
      "\"N-no.\" \"Then I shan't!\" She looked up quickly, her blue eyes very persuasive. \"I don't very often h\n",
      "Number of words in the sample: 12585\n",
      "----------\n",
      "I do not preach next Sunday! SIR TRISTRAM. You'd better not! No, I'm here for the races. THE DEAN. T\n",
      "Number of words in the sample: 15013\n",
      "----------\n",
      "It may have been the instinct of despair that led that Prince to appeal again to Gordon, but the Dar\n",
      "Number of words in the sample: 20289\n",
      "----------\n",
      "The piece on Gouverneur Morris's Oration on Hamilton and that on the Louisiana Memorial are the last\n",
      "Number of words in the sample: 18681\n",
      "----------\n",
      "[12] Chapter XVII. A much more impressive disaster, both in its dramatic features and as illustratin\n",
      "Number of words in the sample: 24342\n",
      "----------\n",
      "She did not seek to combat her love; to what purpose should she do so? No one would ever know it. He\n",
      "Number of words in the sample: 20842\n",
      "----------\n",
      "There are four things which travellers ought never to lose: their luggage, their temper, their healt\n",
      "Number of words in the sample: 12214\n",
      "----------\n",
      "All matters relating to this subject are to be found in the Compendiums for 1693, 1694, and 1695. Su\n",
      "Number of words in the sample: 23339\n",
      "----------\n",
      "panted Caven. \"How do you like that?\" And he held Tom with one hand and hit him in the neck with the\n",
      "Number of words in the sample: 14280\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# random.seed(144)\n",
    "\n",
    "texts = []\n",
    "min_chunk_size = 10  # Set the minimum chunk size (in sentences)\n",
    "for _ in tqdm(range(1000)):\n",
    "    n_words = 0\n",
    "    while n_words < 12000:\n",
    "        text = random.choice(original_ds)[\"text\"]\n",
    "        n_words = len(text.split())\n",
    "    chunk = get_random_chunk(text, min_chunk_size)\n",
    "    texts.append(chunk)\n",
    "\n",
    "for text in texts[:10]:\n",
    "    print(text[:100])    \n",
    "    print(f\"Number of words in the sample: {len(text.split())}\")\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a14e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Page SOME ANIMAL PROPENSITIES. 81 THE PETRIFIE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"N-no.\" \"Then I shan't!\" She looked up quickly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I do not preach next Sunday! SIR TRISTRAM. You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It may have been the instinct of despair that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The piece on Gouverneur Morris's Oration on Ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Page SOME ANIMAL PROPENSITIES. 81 THE PETRIFIE...\n",
       "1  \"N-no.\" \"Then I shan't!\" She looked up quickly...\n",
       "2  I do not preach next Sunday! SIR TRISTRAM. You...\n",
       "3  It may have been the instinct of despair that ...\n",
       "4  The piece on Gouverneur Morris's Oration on Ha..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save locally\n",
    "dst_path = Path(\"../datasets/pg19_valid_1k_chunks.csv\")\n",
    "\n",
    "df = pd.DataFrame({\"text\": texts})\n",
    "df.to_csv(dst_path)\n",
    "\n",
    "df = pd.read_csv(dst_path, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b370208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  2.02ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 57.7MB / 57.7MB, 12.0MB/s  \n",
      "New Data Upload: 100%|██████████| 57.7MB / 57.7MB, 12.0MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.96s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/LarryLovestein/pg19_1k/commit/aaeb326aa0c5b3983c42fc0663f84dc66c3ab3d5', commit_message='Upload dataset', commit_description='', oid='aaeb326aa0c5b3983c42fc0663f84dc66c3ab3d5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/LarryLovestein/pg19_1k', endpoint='https://huggingface.co', repo_type='dataset', repo_id='LarryLovestein/pg19_1k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to hugging face hub\n",
    "dataset = datasets.Dataset.from_pandas(df, preserve_index=False)\n",
    "dataset.push_to_hub(\"LarryLovestein/pg19_1k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e5d48",
   "metadata": {},
   "source": [
    "# Fanfics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5a5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(\"./fanfics_urls.txt\")) as file:\n",
    "    fanfics_urls = list(map(str.strip, file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed5ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_preface_info(html_file):\n",
    "    \"\"\"\n",
    "    Extract title, author, summary, and notes from a single chapter HTML file.\n",
    "    \"\"\"\n",
    "    with open(html_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    \n",
    "    workskin = soup.find(\"div\", id=\"workskin\")\n",
    "    if not workskin:\n",
    "        return {\"title\": None, \"author\": None, \"summary\": None, \"notes\": None}\n",
    "\n",
    "    preface_div = workskin.find(\"div\", class_=\"preface group\")\n",
    "    if not preface_div:\n",
    "        return {\"title\": None, \"author\": None, \"summary\": None, \"notes\": None}\n",
    "\n",
    "    # title\n",
    "    title_tag = preface_div.find(\"h2\", class_=\"title heading\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "    # author\n",
    "    author_tag = preface_div.find(\"a\", rel=\"author\")\n",
    "    author = author_tag.get_text(strip=True) if author_tag else None\n",
    "\n",
    "    # summary\n",
    "    summary = None\n",
    "    summary_div = preface_div.find(\"div\", class_=\"summary module\")\n",
    "    if summary_div:\n",
    "        blockquote = summary_div.find(\"blockquote\", class_=\"userstuff\")\n",
    "        if blockquote:\n",
    "            summary = blockquote.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    # notes\n",
    "    notes = None\n",
    "    notes_div = preface_div.find(\"div\", class_=\"notes module\")\n",
    "    if notes_div:\n",
    "        blockquote = notes_div.find(\"blockquote\", class_=\"userstuff\")\n",
    "        if blockquote:\n",
    "            notes = blockquote.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"author\": author,\n",
    "        \"summary\": summary,\n",
    "        \"notes\": notes\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_book_text(html_file):\n",
    "    # 1. Read the HTML\n",
    "    with open(html_file, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    # 2. Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step A: Remove \"Chapter Notes\" paragraphs and the blockquotes\n",
    "    #         within any <div class=\"meta group\">\n",
    "    # ----------------------------------------------------------------\n",
    "    meta_divs = soup.find_all(\"div\", class_=\"meta\")\n",
    "    for meta_div in meta_divs:\n",
    "        # Find the <p> that exactly or partially matches \"Chapter Notes\"\n",
    "        notes_p = meta_div.find(\"p\", string=lambda text: text and \"Chapter Notes\" in text)\n",
    "        if notes_p:\n",
    "            notes_p.decompose()\n",
    "        \n",
    "        # Also remove the <blockquote> (class=\"userstuff\") under <div class=\"meta group\">\n",
    "        blockquote = meta_div.find(\"blockquote\", class_=\"userstuff\")\n",
    "        if blockquote:\n",
    "            blockquote.decompose()\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step B: Collect the text we DO want in the order it appears:\n",
    "    #   - <h2 class=\"heading\"> (the chapter titles)\n",
    "    #   - <div class=\"userstuff\"><p> ... </p></div> (the main story text)\n",
    "    #\n",
    "    # We'll iterate through all tags in document order and pick out\n",
    "    # only those that match our criteria.\n",
    "    # ----------------------------------------------------------------\n",
    "    extracted_text = []\n",
    "\n",
    "    for tag in soup.find_all():\n",
    "        # 1) If it's a heading\n",
    "        if tag.name == \"h2\" and \"heading\" in tag.get(\"class\", []):\n",
    "            heading_text = tag.get_text(strip=True)\n",
    "            if heading_text:\n",
    "                extracted_text += [f'\\n{heading_text}\\n']\n",
    "\n",
    "        # 2) If it's a <p> under a <div class=\"userstuff\">\n",
    "        elif (tag.name == \"p\" \n",
    "              and tag.parent \n",
    "              and tag.parent.name == \"div\"\n",
    "              and \"userstuff\" in tag.parent.get(\"class\", [])):\n",
    "            p_text = tag.get_text(strip=False)\n",
    "            if p_text:\n",
    "                extracted_text.append(p_text)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step C: Return/join the final cleaned text\n",
    "    # ----------------------------------------------------------------\n",
    "    return \"\\n\".join(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e2ec270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ../datasets/fanfics/A_Great_Eye_lidless.html\n",
      "Path: ../datasets/fanfics/Adagio.html\n",
      "Path: ../datasets/fanfics/Children_of_the_Desert.html\n",
      "Path: ../datasets/fanfics/Christmas_and.html\n",
      "Path: ../datasets/fanfics/Creatures_of_Truth.html\n",
      "Path: ../datasets/fanfics/Grogu_Tells_Stories.html\n",
      "Path: ../datasets/fanfics/Harry_Potter_and_the.html\n",
      "Path: ../datasets/fanfics/I_Know_Where_the_Stars.html\n",
      "Path: ../datasets/fanfics/In_Which_Harry_and_Ladon.html\n",
      "Path: ../datasets/fanfics/Jay_Baby.html\n",
      "Path: ../datasets/fanfics/Laws_of_the_Sea.html\n",
      "Path: ../datasets/fanfics/Mirror_Prism.html\n",
      "Path: ../datasets/fanfics/People_Stained_With.html\n",
      "Path: ../datasets/fanfics/Shattered_Pieces_of_the.html\n",
      "Path: ../datasets/fanfics/Sweet_Creatures.html\n",
      "Path: ../datasets/fanfics/The_Bot_the_World_Forgot.html\n",
      "Path: ../datasets/fanfics/The_Last_of_the_Jedi.html\n",
      "Path: ../datasets/fanfics/The_Resurrection_of.html\n",
      "Path: ../datasets/fanfics/The_Silmarillion_Simplified.html\n",
      "Path: ../datasets/fanfics/The_Sith_Strikes_Back.html\n"
     ]
    }
   ],
   "source": [
    "# Convert HTML to foramtted text files\n",
    "fanfics_path = Path(\"../datasets/fanfics/\")\n",
    "fanfics_path.mkdir(exist_ok=True)\n",
    "fanfics_clean_path = Path(\"../datasets/fanfics_clean/\")\n",
    "fanfics_clean_path.mkdir(exist_ok=True)\n",
    "for fanfic_path in fanfics_path.glob('*.html'):\n",
    "    print(\"Path:\", fanfic_path)\n",
    "    book_text = extract_book_text(fanfic_path)\n",
    "    preface = extract_preface_info(fanfic_path)\n",
    "    with open(f\"{fanfics_clean_path / fanfic_path.stem}.txt\", \"w\") as file:\n",
    "        for header in [\"Title\", \"Author\", \"Summary\", \"Notes\"]:\n",
    "            if preface[header.lower()]:\n",
    "                file.write(f\"{header}: {preface[header.lower()]}\\n\\n\")\n",
    "        file.write(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(text: str, min_chunk_size: int) -> str:\n",
    "    # random.seed(144)\n",
    "    # Erase all the spaces\n",
    "    text = \" \".join(text.split())\n",
    "    # Split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) < min_chunk_size:\n",
    "        chunk = \" \".join(sentences)\n",
    "    else:\n",
    "        chunk = \"\"\n",
    "        n_words = 0\n",
    "        # Iterate untill appropriate chunk is found\n",
    "        while not (12000 < n_words < 25000):\n",
    "            max_start = len(sentences) - min_chunk_size\n",
    "            start = random.randint(0, max_start)\n",
    "            end = random.randint(start + min_chunk_size, len(sentences))\n",
    "            chunk = \" \".join(sentences[start:end])\n",
    "            n_words = len(chunk.split())\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07114768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:00<00:00, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you really wish for such control over living beings?\" \"I don't,\" replied Aule. \"I had just wanted\n",
      "Number of words in the sample: 12313\n",
      "----------\n",
      "“We know.” Cesta turned to Maeglin and held out her hand. He quickly crossed to her side, not notici\n",
      "Number of words in the sample: 13245\n",
      "----------\n",
      "However hard it was, however long it took, Harry would master it. * “BOLLOCKS!” Harry blinked and lo\n",
      "Number of words in the sample: 23271\n",
      "----------\n",
      "In Sauron's dungeon, Beren and Finrod were the only ones still alive out of their companions. Sauron\n",
      "Number of words in the sample: 15344\n",
      "----------\n",
      "Harry smiled and turned back to the book in front of him, pulling out a clean set of parchment and a\n",
      "Number of words in the sample: 22452\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for fanfic_path in fanfics_clean_path.glob(\"*.txt\"):\n",
    "    with open(fanfic_path) as file:\n",
    "        data += [file.read()]\n",
    "\n",
    "\n",
    "texts = []\n",
    "min_chunk_size = 10  # Set the minimum chunk size (in sentences)\n",
    "for _ in tqdm(range(1000)):\n",
    "    n_words = 0\n",
    "    while n_words < 12000:\n",
    "        text = random.choice(data)\n",
    "        n_words = len(text.split())\n",
    "    chunk = get_random_chunk(text, min_chunk_size)\n",
    "    texts.append(chunk)\n",
    "\n",
    "for text in texts[:5]:\n",
    "    print(text[:100])\n",
    "    print(f\"Number of words in the sample: {len(text.split())}\")\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bd36a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you really wish for such control over livin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“We know.” Cesta turned to Maeglin and held ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>However hard it was, however long it took, Har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In Sauron's dungeon, Beren and Finrod were the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry smiled and turned back to the book in fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Do you really wish for such control over livin...\n",
       "1  “We know.” Cesta turned to Maeglin and held ou...\n",
       "2  However hard it was, however long it took, Har...\n",
       "3  In Sauron's dungeon, Beren and Finrod were the...\n",
       "4  Harry smiled and turned back to the book in fr..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save locally\n",
    "dst_path = Path(\"../datasets/fanfics_1k_chunks.csv\")\n",
    "\n",
    "df = pd.DataFrame({\"text\": texts})\n",
    "df.to_csv(dst_path)\n",
    "\n",
    "df = pd.read_csv(dst_path, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ba377a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00,  3.92ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 57.8MB / 57.8MB, 12.1MB/s  \n",
      "New Data Upload: 100%|██████████| 57.8MB / 57.8MB, 12.1MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:07<00:00,  7.04s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/LarryLovestein/fanfics_1k/commit/bc46a1efd6a8c5e02e8e5e20fcff0dac976552b3', commit_message='Upload dataset', commit_description='', oid='bc46a1efd6a8c5e02e8e5e20fcff0dac976552b3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/LarryLovestein/fanfics_1k', endpoint='https://huggingface.co', repo_type='dataset', repo_id='LarryLovestein/fanfics_1k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to hugging face hub\n",
    "dataset = datasets.Dataset.from_pandas(df, preserve_index=False)\n",
    "dataset.push_to_hub(\"LarryLovestein/fanfics_1k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c6e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compression_horizon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
