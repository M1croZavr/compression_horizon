{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "fa180eBPZRRC",
        "Mj8kml4plT8f",
        "wvWYTWu2JhwM"
      ],
      "gpuType": "T4",
      "mount_file_id": "1hAXcU_UMO5aH1JktJz-WP-CKlMENJk-H",
      "authorship_tag": "ABX9TyOpJ860wrWcYj789C4/6KyW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M1croZavr/compression_horizon/blob/task%2Fhybrid_loss/notebooks/Compression_hybrid_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling, AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "guZNmrc5UCCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    subprocess.check_output([\"nvidia-smi\"], shell=True)\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"nvidia-smi is not available\")"
      ],
      "metadata": {
        "id": "4L5BBf1oUaYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments launching"
      ],
      "metadata": {
        "id": "b5c3kR1s9ui-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "# %tensorboard --logdir=/content/compression_horizon/artifacts/experiments/common_loss\n",
        "%tensorboard --logdir=/content/compression_horizon/artifacts/experiments/hybrid_loss"
      ],
      "metadata": {
        "id": "eD3R8Pv_9xMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch task/hybrid_loss https://github.com/M1croZavr/compression_horizon.git"
      ],
      "metadata": {
        "id": "IDKuDkZh4Cov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common loss launches"
      ],
      "metadata": {
        "id": "fa180eBPZRRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 4 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100"
      ],
      "metadata": {
        "id": "T1Rr8R35P-RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 32 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100"
      ],
      "metadata": {
        "id": "mQTcbxnEb0Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100"
      ],
      "metadata": {
        "id": "f97sUuBlcgBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R /content/compression_horizon/artifacts/experiments/common_loss ./drive/MyDrive/compression_horizon/"
      ],
      "metadata": {
        "id": "-QyXdLFZdz61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid loss launches"
      ],
      "metadata": {
        "id": "hrt0UmdkZyJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 4 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type l2 --hybrid_alpha 0.2 --num_alignment_layers 1"
      ],
      "metadata": {
        "id": "FaCe5kv59uLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 32 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type l2 --hybrid_alpha 0.2 --num_alignment_layers 1"
      ],
      "metadata": {
        "id": "evcS73Ad9uJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type l2 --hybrid_alpha 0.2 --num_alignment_layers 1"
      ],
      "metadata": {
        "id": "akLNidEiBOBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type cosine --hybrid_alpha 0.2 --num_alignment_layers 1"
      ],
      "metadata": {
        "id": "kP2OWJfCF1zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type l1 --hybrid_alpha 0.2 --num_alignment_layers 1"
      ],
      "metadata": {
        "id": "lQ4W1inPI-Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type cosine --hybrid_alpha 0.2 --num_alignment_layers 3"
      ],
      "metadata": {
        "id": "jy6y7RKeNsgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type cosine --hybrid_alpha 0.3 --num_alignment_layers 5"
      ],
      "metadata": {
        "id": "wqohHPltRHAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ..."
      ],
      "metadata": {
        "id": "w3RIjsPwBtnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CE comparison"
      ],
      "metadata": {
        "id": "Mj8kml4plT8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running device:\", device)"
      ],
      "metadata": {
        "id": "cgKC8Quvmagj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "checkpoint = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.float32).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token});"
      ],
      "metadata": {
        "id": "98PsCFQ3ndyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exactly sample indexed 0 as we trained on it\n",
        "raw_dataset = load_dataset(\"mrsndmn/pg19\", split=\"test\")\n",
        "train_dataset = raw_dataset.select(range(1))\n",
        "example = tokenizer(train_dataset[0][\"text\"], truncation=True, max_length=4, return_tensors=\"pt\")\n",
        "input_ids = example[\"input_ids\"].to(device)\n",
        "attention_mask = example[\"attention_mask\"].to(device)"
      ],
      "metadata": {
        "id": "lAGOj474lelH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = load_from_disk(\n",
        "    \"/content/drive/MyDrive/compression_horizon/l2_None_0_4_666004aa-d739-42a9-8f39-15a11466c4f8/compressed_prefixes\"\n",
        ")\n",
        "compressed_embeddings = torch.FloatTensor(result[0][\"embedding\"]).unsqueeze(dim=0).to(device)\n",
        "with torch.no_grad():\n",
        "    sequence_embeddings = model.model.embed_tokens(input_ids)\n",
        "united_embeddings = torch.cat(\n",
        "    (compressed_embeddings, sequence_embeddings),\n",
        "    dim=1,\n",
        ")\n",
        "united_attention_mask = torch.cat(\n",
        "    (torch.tensor([[1]]).to(device), attention_mask),\n",
        "    dim=1,\n",
        ")"
      ],
      "metadata": {
        "id": "mG930EVwlejH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        inputs_embeds=united_embeddings,\n",
        "        attention_mask=attention_mask,\n",
        "    )"
      ],
      "metadata": {
        "id": "RqYiq3VNlS5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y2OFA-h5mU2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.functional.cross_entropy(outputs.logits[:, :-1, :].flatten(0, 1), input_ids.flatten()).item()"
      ],
      "metadata": {
        "id": "0RmFjo5np4iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation outside the compressed sequence"
      ],
      "metadata": {
        "id": "wvWYTWu2JhwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running device:\", device)"
      ],
      "metadata": {
        "id": "NDhT6f88KJab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "checkpoint = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.float32).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token});"
      ],
      "metadata": {
        "id": "Zl-ta7HBKRu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = load_from_disk(\n",
        "    \"/content/drive/MyDrive/compression_horizon/l2_None_0_4_666004aa-d739-42a9-8f39-15a11466c4f8/compressed_prefixes\"\n",
        ")\n",
        "compressed_embeddings = torch.FloatTensor(result[0][\"embedding\"]).unsqueeze(dim=0).to(device)"
      ],
      "metadata": {
        "id": "M6eJT_EsKRpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_from_compression(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    compressed_embeddings: torch.Tensor,  # [1, mem, hidden]\n",
        "    max_new_tokens: int,\n",
        "    num_return_sequences: int = 1,\n",
        ") -> list[str]:\n",
        "    # Cast to the same device\n",
        "    device = compressed_embeddings.device\n",
        "    if model.device != device:\n",
        "        model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Add pad_token to a tokenizer\n",
        "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
        "        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    # Prepare batch of prefixes\n",
        "    if num_return_sequences > 1:\n",
        "        compressed_embeddings = compressed_embeddings.expand(num_return_sequences, -1, -1)  # [batch, mem, hidden]\n",
        "    batch_size, num_compression_tokens, hidden_size = compressed_embeddings.shape\n",
        "\n",
        "    # Container for generated token ids\n",
        "    generated_token_ids = torch.empty((batch_size, 0), dtype=torch.long, device=device)\n",
        "    # Model's input embedding layer\n",
        "    input_embeddings = model.get_input_embeddings()\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Embeddings\n",
        "        if generated_token_ids.size(1) == 0:\n",
        "            generated_embeddings = torch.empty(batch_size, 0, hidden_size, device=device)\n",
        "        else:\n",
        "            generated_embeddings = input_embeddings(generated_token_ids)  # [batch, sequence, hidden]\n",
        "        united_token_embeddings = torch.cat([compressed_embeddings, generated_embeddings], dim=1)  # [batch, mem + sequence, hidden]\n",
        "\n",
        "        # Attention mask\n",
        "        compression_attention_mask = torch.ones(\n",
        "            (batch_size, num_compression_tokens),\n",
        "            dtype=torch.long,\n",
        "            device=device\n",
        "        )\n",
        "        attention_mask = torch.ones(\n",
        "            (batch_size, generated_embeddings.size(1)),\n",
        "            dtype=torch.long,\n",
        "            device=device\n",
        "        )\n",
        "        united_attention_mask = torch.cat(\n",
        "            (compression_attention_mask, attention_mask),\n",
        "            dim=1\n",
        "        )  # [batch, mem + sequence]\n",
        "\n",
        "        outputs = model(inputs_embeds=united_token_embeddings, attention_mask=united_attention_mask)\n",
        "        logits = outputs.logits[:, -1, :]  # [batch, vocabulary]\n",
        "\n",
        "        next_token_ids = torch.argmax(logits, dim=-1)  # [batch]\n",
        "\n",
        "        # If a sequence already reached EOS token leave EOS to the end\n",
        "        if eos_token_id is not None:\n",
        "            if generated_token_ids.size(1) > 0:\n",
        "                reached_eos = generated_token_ids[:, -1].eq(eos_token_id)\n",
        "                next_token_ids = torch.where(reached_eos, torch.full_like(next_token_ids, eos_token_id), next_token_ids)\n",
        "\n",
        "        generated_token_ids = torch.cat([generated_token_ids, next_token_ids.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        # Stop early if all sequences just produced eos and had eos previously\n",
        "        if eos_token_id is not None and torch.all(next_token_ids.eq(eos_token_id)):\n",
        "            break\n",
        "\n",
        "    texts = tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "lFehbpk_T8bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1I7YSSqCTDtG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}