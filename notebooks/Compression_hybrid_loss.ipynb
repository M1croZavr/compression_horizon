{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/M1croZavr/compression_horizon/blob/task%2Fhybrid_loss/notebooks/Compression_hybrid_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guZNmrc5UCCQ"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4L5BBf1oUaYQ"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    subprocess.check_output([\"nvidia-smi\"], shell=True)\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"nvidia-smi is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5c3kR1s9ui-"
   },
   "source": [
    "# Experiments launching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eD3R8Pv_9xMr"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "# %tensorboard --logdir=/content/compression_horizon/artifacts/experiments/common_loss\n",
    "%tensorboard --logdir=/content/compression_horizon/artifacts/experiments/hybrid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDKuDkZh4Cov"
   },
   "outputs": [],
   "source": [
    "!git clone --branch task/hybrid_loss https://github.com/M1croZavr/compression_horizon.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa180eBPZRRC"
   },
   "source": [
    "## Common loss launches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1Rr8R35P-RO"
   },
   "outputs": [],
   "source": [
    "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 4 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQTcbxnEb0Us"
   },
   "outputs": [],
   "source": [
    "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 32 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f97sUuBlcgBN"
   },
   "outputs": [],
   "source": [
    "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QyXdLFZdz61"
   },
   "outputs": [],
   "source": [
    "!cp -R /content/compression_horizon/artifacts/experiments/common_loss ./drive/MyDrive/compression_horizon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrt0UmdkZyJY"
   },
   "source": [
    "## Hybrid loss launches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaCe5kv59uLs"
   },
   "outputs": [],
   "source": [
    "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 4 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type l2 --hybrid_alpha 0.2 --num_alignment_layers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evcS73Ad9uJZ"
   },
   "outputs": [],
   "source": [
    "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 32 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type l2 --hybrid_alpha 0.2 --num_alignment_layers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akLNidEiBOBt"
   },
   "outputs": [],
   "source": [
    "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type l2 --hybrid_alpha 0.2 --num_alignment_layers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kP2OWJfCF1zC"
   },
   "outputs": [],
   "source": [
    "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type cosine --hybrid_alpha 0.2 --num_alignment_layers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQ4W1inPI-Jo"
   },
   "outputs": [],
   "source": [
    "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type l1 --hybrid_alpha 0.2 --num_alignment_layers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jy6y7RKeNsgx"
   },
   "outputs": [],
   "source": [
    "!cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type cosine --hybrid_alpha 0.2 --num_alignment_layers 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqohHPltRHAg"
   },
   "outputs": [],
   "source": [
    "# !cd ./compression_horizon/; uv run python scripts/hybrid_loss.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --learning_rate 0.01 --max_sequence_length 128 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type cosine --hybrid_alpha 0.3 --num_alignment_layers 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3RIjsPwBtnk"
   },
   "outputs": [],
   "source": [
    "!cp ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj8kml4plT8f"
   },
   "source": [
    "# CE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgKC8Quvmagj"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98PsCFQ3ndyi"
   },
   "outputs": [],
   "source": [
    "# checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.float32).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAGOj474lelH"
   },
   "outputs": [],
   "source": [
    "# Exactly sample indexed 0 as we trained on it\n",
    "raw_dataset = load_dataset(\"mrsndmn/pg19\", split=\"test\")\n",
    "train_dataset = raw_dataset.select(range(1))\n",
    "example = tokenizer(train_dataset[0][\"text\"], truncation=True, max_length=4, return_tensors=\"pt\")\n",
    "input_ids = example[\"input_ids\"].to(device)\n",
    "attention_mask = example[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mG930EVwlejH"
   },
   "outputs": [],
   "source": [
    "result = load_from_disk(\n",
    "    \"/content/drive/MyDrive/compression_horizon/l2_None_0_4_666004aa-d739-42a9-8f39-15a11466c4f8/compressed_prefixes\"\n",
    ")\n",
    "compressed_embeddings = torch.FloatTensor(result[0][\"embedding\"]).unsqueeze(dim=0).to(device)\n",
    "with torch.no_grad():\n",
    "    sequence_embeddings = model.model.embed_tokens(input_ids)\n",
    "united_embeddings = torch.cat(\n",
    "    (compressed_embeddings, sequence_embeddings),\n",
    "    dim=1,\n",
    ")\n",
    "united_attention_mask = torch.cat(\n",
    "    (torch.tensor([[1]]).to(device), attention_mask),\n",
    "    dim=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqYiq3VNlS5m"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        inputs_embeds=united_embeddings,\n",
    "        attention_mask=attention_mask,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2OFA-h5mU2x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RmFjo5np4iW"
   },
   "outputs": [],
   "source": [
    "torch.nn.functional.cross_entropy(outputs.logits[:, :-1, :].flatten(0, 1), input_ids.flatten()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvWYTWu2JhwM"
   },
   "source": [
    "# Generation outside the compressed sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDhT6f88KJab"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zl-ta7HBKRu9"
   },
   "outputs": [],
   "source": [
    "# checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.float32).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6eJT_EsKRpl"
   },
   "outputs": [],
   "source": [
    "result = load_from_disk(\n",
    "    \"/content/drive/MyDrive/compression_horizon/l2_None_0_4_666004aa-d739-42a9-8f39-15a11466c4f8/compressed_prefixes\"\n",
    ")\n",
    "compressed_embeddings = torch.FloatTensor(result[0][\"embedding\"]).unsqueeze(dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFehbpk_T8bT"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_from_compression(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    compressed_embeddings: torch.Tensor,  # [1, mem, hidden]\n",
    "    max_new_tokens: int,\n",
    "    num_return_sequences: int = 1,\n",
    ") -> list[str]:\n",
    "    # Cast to the same device\n",
    "    device = compressed_embeddings.device\n",
    "    if model.device != device:\n",
    "        model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Add pad_token to a tokenizer\n",
    "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Prepare batch of prefixes\n",
    "    if num_return_sequences > 1:\n",
    "        compressed_embeddings = compressed_embeddings.expand(num_return_sequences, -1, -1)  # [batch, mem, hidden]\n",
    "    batch_size, num_compression_tokens, hidden_size = compressed_embeddings.shape\n",
    "\n",
    "    # Container for generated token ids\n",
    "    generated_token_ids = torch.empty((batch_size, 0), dtype=torch.long, device=device)\n",
    "    # Model's input embedding layer\n",
    "    input_embeddings = model.get_input_embeddings()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Embeddings\n",
    "        if generated_token_ids.size(1) == 0:\n",
    "            generated_embeddings = torch.empty(batch_size, 0, hidden_size, device=device)\n",
    "        else:\n",
    "            generated_embeddings = input_embeddings(generated_token_ids)  # [batch, sequence, hidden]\n",
    "        united_token_embeddings = torch.cat(\n",
    "            [compressed_embeddings, generated_embeddings], dim=1\n",
    "        )  # [batch, mem + sequence, hidden]\n",
    "\n",
    "        # Attention mask\n",
    "        compression_attention_mask = torch.ones((batch_size, num_compression_tokens), dtype=torch.long, device=device)\n",
    "        attention_mask = torch.ones((batch_size, generated_embeddings.size(1)), dtype=torch.long, device=device)\n",
    "        united_attention_mask = torch.cat((compression_attention_mask, attention_mask), dim=1)  # [batch, mem + sequence]\n",
    "\n",
    "        outputs = model(inputs_embeds=united_token_embeddings, attention_mask=united_attention_mask)\n",
    "        logits = outputs.logits[:, -1, :]  # [batch, vocabulary]\n",
    "\n",
    "        next_token_ids = torch.argmax(logits, dim=-1)  # [batch]\n",
    "\n",
    "        # If a sequence already reached EOS token leave EOS to the end\n",
    "        if eos_token_id is not None:\n",
    "            if generated_token_ids.size(1) > 0:\n",
    "                reached_eos = generated_token_ids[:, -1].eq(eos_token_id)\n",
    "                next_token_ids = torch.where(reached_eos, torch.full_like(next_token_ids, eos_token_id), next_token_ids)\n",
    "\n",
    "        generated_token_ids = torch.cat([generated_token_ids, next_token_ids.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        # Stop early if all sequences just produced eos and had eos previously\n",
    "        if eos_token_id is not None and torch.all(next_token_ids.eq(eos_token_id)):\n",
    "            break\n",
    "\n",
    "    texts = tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I7YSSqCTDtG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOpJ860wrWcYj789C4/6KyW",
   "collapsed_sections": [
    "fa180eBPZRRC",
    "Mj8kml4plT8f",
    "wvWYTWu2JhwM"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "mount_file_id": "1hAXcU_UMO5aH1JktJz-WP-CKlMENJk-H",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}