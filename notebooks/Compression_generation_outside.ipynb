{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M1croZavr/compression_horizon/blob/task%2Fgeneration_outside/notebooks/Compression_generation_outside.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guZNmrc5UCCQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset, Dataset\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6LS8JZEHUWYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAC3zQmr3yxX"
      },
      "outputs": [],
      "source": [
        "!git clone --branch task/generation_outside https://github.com/M1croZavr/compression_horizon.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launches"
      ],
      "metadata": {
        "id": "svtgDggbxwkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=/content/compression_horizon/artifacts/experiments/"
      ],
      "metadata": {
        "id": "922iuWuczWqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 16"
      ],
      "metadata": {
        "id": "3c7yMlHkVevN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run --no-dev python scripts/activation_distillation.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --limit_dataset_items 16 --learning_rate 0.01 --max_sequence_length 16 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100"
      ],
      "metadata": {
        "id": "HEmUB289Vf7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run --no-dev python scripts/activation_distillation.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --limit_dataset_items 16 --learning_rate 0.01 --max_sequence_length 16 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type cosine --hybrid_alpha 1 --num_alignment_layers 5"
      ],
      "metadata": {
        "id": "6FbHJ9wOVf4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 32"
      ],
      "metadata": {
        "id": "24XuAv1iVYSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run --no-dev python scripts/activation_distillation.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --limit_dataset_items 16 --learning_rate 0.01 --max_sequence_length 32 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100"
      ],
      "metadata": {
        "id": "QZM9ncRFxvix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run --no-dev python scripts/activation_distillation.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --limit_dataset_items 16 --learning_rate 0.01 --max_sequence_length 32 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type cosine --hybrid_alpha 1 --num_alignment_layers 5"
      ],
      "metadata": {
        "id": "VTZKLf4nx1VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 64"
      ],
      "metadata": {
        "id": "tHjVlzgfjqlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run --no-dev python scripts/activation_distillation.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --limit_dataset_items 16 --learning_rate 0.01 --max_sequence_length 64 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100"
      ],
      "metadata": {
        "id": "4NXntm02jtF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./compression_horizon/; uv run --no-dev python scripts/activation_distillation.py --model_checkpoint HuggingFaceTB/SmolLM2-1.7B --limit_dataset_items 16 --learning_rate 0.01 --max_sequence_length 64 --number_of_mem_tokens 1 --max_optimization_steps_per_sample 1000 --warmup_steps 100 --loss_type cosine --hybrid_alpha 1 --num_alignment_layers 5"
      ],
      "metadata": {
        "id": "LCZSpiHgjtDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/compression_horizon/artifacts/experiments/* /content/drive/MyDrive/compression_horizon/20-12-2025/"
      ],
      "metadata": {
        "id": "BcyaA8f0x1TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj8kml4plT8f"
      },
      "source": [
        "# Generation outside the compressed sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_from_disk\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
        "\n",
        "from compression_horizon.src.compression_horizon.inference.generation import generate_from_compression\n",
        "from compression_horizon.src.compression_horizon.inference.load import load_compression_embeddings\n",
        "from compression_horizon.src.compression_horizon.metric import calculate_perplexity, calculate_perplexity_logits"
      ],
      "metadata": {
        "id": "o3w4jEhjlWU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_dataset(\n",
        "    result_dataset: Dataset,\n",
        "    model: PreTrainedModel,\n",
        "    tokenizer: PreTrainedTokenizerFast | PreTrainedTokenizer,\n",
        ") -> pd.DataFrame:\n",
        "    records = []\n",
        "    for i, sample in enumerate(result_dataset, start=1):\n",
        "        sample_text = sample[\"text\"]\n",
        "        max_length = sample[\"num_input_tokens\"]\n",
        "        tokenized_sample = tokenizer(sample_text, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "        input_ids = tokenized_sample[\"input_ids\"].to(device)\n",
        "        attention_mask = tokenized_sample[\"attention_mask\"].to(device)\n",
        "        with torch.no_grad():\n",
        "            sequence_embeddings = model.get_input_embeddings()(input_ids)\n",
        "\n",
        "        compressed_embeddings = torch.FloatTensor(sample[\"embedding\"]).unsqueeze(dim=0).to(device)\n",
        "        restored_text = generate_from_compression(model, tokenizer, compressed_embeddings, max_length, 1)[0]\n",
        "\n",
        "        perplexity_logits_score = calculate_perplexity_logits(\n",
        "            model,\n",
        "            compressed_embeddings,\n",
        "            input_ids,\n",
        "            sequence_embeddings,\n",
        "            attention_mask,\n",
        "        )\n",
        "        perplexity_score = calculate_perplexity(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            compressed_embeddings,\n",
        "            sequence_embeddings,\n",
        "            attention_mask,\n",
        "            n=max_length,\n",
        "        )\n",
        "\n",
        "        records.append(\n",
        "            {\n",
        "                \"original_text\": sample_text,\n",
        "                \"restored_text\": restored_text,\n",
        "                \"n_tokens\": max_length,\n",
        "                \"convergence\": sample[\"final_convergence\"],\n",
        "                \"perplexity\": perplexity_logits_score,\n",
        "                \"forward_perplexity\": perplexity_score,\n",
        "                \"mean_perplexity\": (perplexity_logits_score + perplexity_score) / 2\n",
        "\n",
        "            }\n",
        "        )\n",
        "\n",
        "    dataframe = pd.DataFrame(records)\n",
        "    return dataframe"
      ],
      "metadata": {
        "id": "uknLB9nc1yca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgKC8Quvmagj"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98PsCFQ3ndyi"
      },
      "outputs": [],
      "source": [
        "# checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "checkpoint = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.float32).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token});"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset = load_dataset(\"mrsndmn/pg19\", split=\"test\")"
      ],
      "metadata": {
        "id": "BBUtAVO_-Hwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9zAsR4Oyign"
      },
      "source": [
        "## Embeddings trained on common loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16"
      ],
      "metadata": {
        "id": "DwtvKKa07KGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_16_path = \"/content/drive/MyDrive/compression_horizon/20-12-2025/model_HuggingFaceTB_SmolLM2-1.7B_mem_1_init_random_seq_len_16/compressed_prefixes\"\n",
        "common_16_result = load_from_disk(common_16_path)\n",
        "common_16_df = evaluate_dataset(common_16_result, model, tokenizer)"
      ],
      "metadata": {
        "id": "9LJwNj8T7Mr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_16_df.head()"
      ],
      "metadata": {
        "id": "XfKveX8f7Mp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 32"
      ],
      "metadata": {
        "id": "KTiyVLASAIqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_32_path = \"/content/drive/MyDrive/compression_horizon/20-12-2025/model_HuggingFaceTB_SmolLM2-1.7B_mem_1_init_random_seq_len_32/compressed_prefixes\"\n",
        "common_32_result = load_from_disk(common_32_path)\n",
        "common_32_df = evaluate_dataset(common_32_result, model, tokenizer)"
      ],
      "metadata": {
        "id": "S9-foyuz-Fka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_32_df.head()"
      ],
      "metadata": {
        "id": "9fnnMtviDEDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 64"
      ],
      "metadata": {
        "id": "3_2HOOJo-kFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_64_path = \"/content/drive/MyDrive/compression_horizon/20-12-2025/model_HuggingFaceTB_SmolLM2-1.7B_mem_1_init_random_seq_len_64/compressed_prefixes\"\n",
        "common_64_result = load_from_disk(common_64_path)\n",
        "common_64_df = evaluate_dataset(common_64_result, model, tokenizer)"
      ],
      "metadata": {
        "id": "txNPqIf0-is_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_64_df.head()"
      ],
      "metadata": {
        "id": "aQRq-iiH-iq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random ids generation"
      ],
      "metadata": {
        "id": "aBs1mYO8-iSx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAGOj474lelH"
      },
      "outputs": [],
      "source": [
        "# # Load the exact sample as in checkpoint (index 0)\n",
        "# sample = raw_dataset.select(range(1))\n",
        "# example = tokenizer(sample[0][\"text\"], truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "# input_ids = example[\"input_ids\"].to(device)\n",
        "# attention_mask = example[\"attention_mask\"].to(device)\n",
        "# with torch.no_grad():\n",
        "#     sequence_embeddings = model.get_input_embeddings()(input_ids)\n",
        "compressed_embeddings = load_compression_embeddings(\n",
        "    \"/content/drive/MyDrive/compression_horizon/common_loss/HuggingFaceTB/\"\n",
        "    \"SmolLM2-1.7B|128|1|bc818cdb-8346-4cf1-beb0-7459ce626638/compressed_prefixes/\",\n",
        "    device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Proper generation\n",
        "print(generate_from_compression(model, tokenizer, compressed_embeddings, 32, 1)[0], end=\"\\n#####\\n\")\n",
        "# Random position_ids generation\n",
        "print(generate_from_compression(model, tokenizer, compressed_embeddings, 32, 1, random_position_ids=True)[0])"
      ],
      "metadata": {
        "id": "9m0tTJiRluIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XvlJ1nRyr8G"
      },
      "source": [
        "## Embeddings trained on hybrid loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16"
      ],
      "metadata": {
        "id": "t15eBOYjAs8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_16_path = \"/content/drive/MyDrive/compression_horizon/20-12-2025/model_HuggingFaceTB_SmolLM2-1.7B_mem_1_ch_cosine_hybrid_alpha_1.0_init_random_seq_len_16/compressed_prefixes\"\n",
        "hybrid_16_result = load_from_disk(hybrid_16_path)\n",
        "hybrid_16_df = evaluate_dataset(hybrid_16_result, model, tokenizer)"
      ],
      "metadata": {
        "id": "0GKnNrRsAu4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_16_df.head()"
      ],
      "metadata": {
        "id": "O5c2JfigAu2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 32"
      ],
      "metadata": {
        "id": "bUBtij6wJDgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_32_path = \"/content/drive/MyDrive/compression_horizon/20-12-2025/model_HuggingFaceTB_SmolLM2-1.7B_mem_1_ch_cosine_hybrid_alpha_1.0_init_random_seq_len_32/compressed_prefixes\"\n",
        "hybrid_32_result = load_from_disk(hybrid_32_path)\n",
        "hybrid_32_df = evaluate_dataset(hybrid_32_result, model, tokenizer)"
      ],
      "metadata": {
        "id": "V-sXCpZrBBMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_32_df.head()"
      ],
      "metadata": {
        "id": "Rg44vwFsBBKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 64"
      ],
      "metadata": {
        "id": "-SpqHi-hBMts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_64_path = \"/content/drive/MyDrive/compression_horizon/20-12-2025/model_HuggingFaceTB_SmolLM2-1.7B_mem_1_ch_cosine_hybrid_alpha_1.0_init_random_seq_len_64/compressed_prefixes\"\n",
        "hybrid_64_result = load_from_disk(hybrid_64_path)\n",
        "hybrid_64_df = evaluate_dataset(hybrid_64_result, model, tokenizer)"
      ],
      "metadata": {
        "id": "OFLOPB7rBM9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_64_df.head()"
      ],
      "metadata": {
        "id": "AKOSgh-9BNP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random ids generation"
      ],
      "metadata": {
        "id": "8XT-waFkBee6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6eJT_EsKRpl"
      },
      "outputs": [],
      "source": [
        "compressed_embeddings = load_compression_embeddings(\n",
        "    \"/content/drive/MyDrive/compression_horizon/17-11-2025/hybrid_loss/HuggingFaceTB/\"\n",
        "    \"SmolLM2-1.7B|128|1|0.01|cosine|1.0|5|978c86f1-57ac-4eba-a446-6a6dc874d451/compressed_prefixes/\",\n",
        "    device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Proper generation\n",
        "print(generate_from_compression(model, tokenizer, compressed_embeddings, 32, 1)[0], end=\"\\n#####\\n\")\n",
        "# Random position_ids generation\n",
        "print(generate_from_compression(model, tokenizer, compressed_embeddings, 32, 1, random_position_ids=True)[0])"
      ],
      "metadata": {
        "id": "cWmoD-VgmG5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hybrid_loss_results_root = pathlib.Path(\"/content/drive/MyDrive/compression_horizon/22-11-2025/hybrid_loss/\")\n",
        "# for result_dir in os.listdir(hybrid_loss_results_root):\n",
        "#     compressed_embeddings = load_compression_embeddings(\n",
        "#         hybrid_loss_results_root / result_dir / \"compressed_prefixes\",\n",
        "#         device,\n",
        "#     )\n",
        "#     perplexity_score = calculate_perplexity(\n",
        "#         model,\n",
        "#         tokenizer,\n",
        "#         compressed_embeddings,\n",
        "#         sequence_embeddings,\n",
        "#         attention_mask,\n",
        "#         n=32,\n",
        "#     )\n",
        "#     print(\n",
        "#         result_dir,\n",
        "#         \"\\n\\tHybrid loss perplexity (n=32):\",\n",
        "#         perplexity_score,\n",
        "#         end=\"\\n\\n\",\n",
        "#     )"
      ],
      "metadata": {
        "id": "3yj2PQK-lnQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hybrid_loss_results_root = pathlib.Path(\"/content/drive/MyDrive/compression_horizon/17-11-2025/hybrid_loss/HuggingFaceTB/\")\n",
        "# for result_dir in os.listdir(hybrid_loss_results_root):\n",
        "#     if \"|128|\" in result_dir:\n",
        "#         try:\n",
        "#             compressed_embeddings = load_compression_embeddings(\n",
        "#                 hybrid_loss_results_root / result_dir / \"compressed_prefixes\",\n",
        "#                 device,\n",
        "#             )\n",
        "#             perplexity_score = calculate_perplexity(\n",
        "#                 model,\n",
        "#                 tokenizer,\n",
        "#                 compressed_embeddings,\n",
        "#                 sequence_embeddings,\n",
        "#                 attention_mask,\n",
        "#                 n=32,\n",
        "#             )\n",
        "#             print(\n",
        "#                 result_dir,\n",
        "#                 \"\\n\\tHybrid loss perplexity (n=32)\",\n",
        "#                 perplexity_score,\n",
        "#                 end=\"\\n\\n\",\n",
        "#             )\n",
        "#         except FileNotFoundError:\n",
        "#             continue"
      ],
      "metadata": {
        "id": "a2i95KvglfWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "xSW2zixvB7_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [\n",
        "    \"n_tokens\",\n",
        "    \"convergence\",\n",
        "    \"perplexity\",\n",
        "    \"forward_perplexity\",\n",
        "    \"mean_perplexity\",\n",
        "]\n",
        "\n",
        "\n",
        "def summarize_df(df):\n",
        "    return df[metrics].agg([\"mean\", \"std\", \"min\", \"max\"])\n",
        "\n",
        "\n",
        "def robust_summary_df(df):\n",
        "    return df[metrics].agg(\n",
        "        mean=\"mean\",\n",
        "        std=\"std\",\n",
        "        median=\"median\",\n",
        "        q25=lambda x: x.quantile(0.25),\n",
        "        q75=lambda x: x.quantile(0.75),\n",
        "    )\n",
        "\n",
        "\n",
        "def tag(df, variant):\n",
        "    df = df.copy()\n",
        "    df[\"setup\"] = variant\n",
        "    return df\n",
        "\n",
        "\n",
        "all_df = pd.concat([\n",
        "    tag(common_16_df, \"common_16\"),\n",
        "    tag(hybrid_16_df, \"hybrid_16\"),\n",
        "    tag(common_32_df, \"common_32\"),\n",
        "    tag(hybrid_32_df, \"hybrid_32\"),\n",
        "    tag(common_64_df, \"common_64\"),\n",
        "    tag(hybrid_64_df, \"hybrid_64\"),\n",
        "])\n",
        "\n",
        "\n",
        "summary_all = (\n",
        "    all_df\n",
        "    .groupby(\"setup\")[metrics]\n",
        "    .agg([\"mean\", \"std\", \"median\"])\n",
        ")\n",
        "\n",
        "summary_all"
      ],
      "metadata": {
        "id": "nJOWP1fcED7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM as a judge"
      ],
      "metadata": {
        "id": "IDb46fPpJHT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai --quiet"
      ],
      "metadata": {
        "id": "zgIAQ8uYKm7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class PrefixRestoreAndContinuationEvaluation(BaseModel):\n",
        "    \"\"\"\n",
        "    Binary evaluation of a model output consisting of a restored prefix\n",
        "    followed by a continuation.\n",
        "\n",
        "    The evaluator is given the original prefix as ground truth.\n",
        "    Evaluation focuses primarily on the continuation quality, with a\n",
        "    lightweight check on restored prefix fidelity.\n",
        "    \"\"\"\n",
        "\n",
        "    prefix_adequate: bool = Field(\n",
        "        description=(\n",
        "            \"True if the restored prefix is semantically equivalent to the \"\n",
        "            \"original prefix, allowing minor paraphrasing or formatting differences.\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    continuation_coherent: bool = Field(\n",
        "        description=(\n",
        "            \"True if the continuation logically follows from the original prefix \"\n",
        "            \"without contradictions.\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    continuation_relevant: bool = Field(\n",
        "        description=(\n",
        "            \"True if the continuation meaningfully stays on-topic with respect \"\n",
        "            \"to the original prefix.\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    continuation_fluent: bool = Field(\n",
        "        description=(\n",
        "            \"True if the continuation is grammatically correct and linguistically natural.\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    style_consistent: bool = Field(\n",
        "        description=(\n",
        "            \"True if the continuation matches the tone, register, and style \"\n",
        "            \"of the original prefix.\"\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "cSX6ypIIJKFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert evaluator of language model outputs. \"\n",
        "            \"You evaluate different performances of LLM to figure out which is the best. \"\n",
        "            \"The task is to evaluate a restored prefix and its continuation \"\n",
        "            \"given the original prefix as reference.\"\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"\"\"\n",
        "You are given:\n",
        "\n",
        "1) The ORIGINAL TEXT PREFIX (ground truth).\n",
        "2) A MODEL OUTPUT consisting of:\n",
        "   - a restored prefix (which may contain minor differences)\n",
        "   - followed by a continuation.\n",
        "\n",
        "Your primary task is to evaluate the QUALITY OF THE CONTINUATION.\n",
        "You should only penalize the restored prefix if it is meaningfully incorrect\n",
        "or alters the intended meaning.\n",
        "\n",
        "Answer True or False for each criterion:\n",
        "\n",
        "- Prefix adequately restored:\n",
        "  Is the restored prefix semantically equivalent to the original prefix,\n",
        "  allowing minor paraphrasing or formatting differences?\n",
        "\n",
        "- Continuation coherent:\n",
        "  Does the continuation logically follow from the original prefix?\n",
        "\n",
        "- Continuation relevant:\n",
        "  Does the continuation stay on-topic with respect to the original prefix?\n",
        "\n",
        "- Continuation fluent:\n",
        "  Is the continuation grammatically correct and natural?\n",
        "\n",
        "- Style consistent:\n",
        "  Does the continuation match the tone and style of the original prefix?\n",
        "\n",
        "ORIGINAL PREFIX:\n",
        "{original_prefix}\n",
        "\n",
        "MODEL OUTPUT (restored prefix + continuation):\n",
        "{generated_text}\n",
        "\"\"\"\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "RJETRUjlJKC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "judge_llm = ChatOpenAI(\n",
        "    model=\"x-ai/grok-4.1-fast\",\n",
        "    temperature=0,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=userdata.get('OpenRouter'),\n",
        ")\n",
        "\n",
        "judge_llm = judge_llm.with_structured_output(PrefixRestoreAndContinuationEvaluation)"
      ],
      "metadata": {
        "id": "f0X8V6GuJKAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(result_dataset: Dataset, model, tokenizer) -> tuple:\n",
        "    for sample in result_dataset:\n",
        "        original_text = sample[\"text\"]\n",
        "        compressed_embeddings = torch.FloatTensor(sample[\"embedding\"]).unsqueeze(dim=0).to(device)\n",
        "        generated_text = generate_from_compression(model, tokenizer, compressed_embeddings, 64, 1)[0]\n",
        "        yield original_text, generated_text\n",
        "\n",
        "\n",
        "starts = []\n",
        "samples_common = []\n",
        "samples_hybrid = []\n",
        "for sample_a in generate_samples(common_32_result, model, tokenizer):\n",
        "    starts.append(sample_a[0])\n",
        "    samples_common.append(sample_a[1])\n",
        "for sample_b in generate_samples(hybrid_32_result, model, tokenizer):\n",
        "    samples_hybrid.append(sample_b[1])\n",
        "\n",
        "samples = [{\"text_start\": starts[i], \"continuation_a\": samples_common[i], \"continuation_b\": samples_hybrid[i]} for i in range(len(starts))]"
      ],
      "metadata": {
        "id": "hD2qysDBMRYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "# with open(\"./samples.pkl\", \"wb\") as file:\n",
        "#     pickle.dump(samples, file)\n",
        "with open(\"./samples.pkl\", \"rb\") as file:\n",
        "    samples = pickle.load(file)"
      ],
      "metadata": {
        "id": "dTqbOgf-JZmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "chain = prompt | judge_llm\n",
        "\n",
        "common_results: List[PrefixRestoreAndContinuationEvaluation] = []\n",
        "hybrid_results: List[PrefixRestoreAndContinuationEvaluation] = []\n",
        "\n",
        "for i, sample in enumerate(samples, start=1):\n",
        "    print(i)\n",
        "    result_common = chain.invoke(\n",
        "        {\n",
        "            \"original_prefix\": sample[\"text_start\"],\n",
        "            \"generated_text\": sample[\"continuation_a\"],\n",
        "        }\n",
        "    )\n",
        "    common_results.append(result_common)\n",
        "    result_hybrid = chain.invoke(\n",
        "        {\n",
        "            \"original_prefix\": sample[\"text_start\"],\n",
        "            \"generated_text\": sample[\"continuation_b\"],\n",
        "        }\n",
        "    )\n",
        "    hybrid_results.append(result_hybrid)\n",
        "\n",
        "# with open(\"./llm_judge.pkl\", \"wb\") as file:\n",
        "#     pickle.dump({\"common\": common_results, \"hybrid\": hybrid_results}, file)"
      ],
      "metadata": {
        "id": "ae8DbM5zLWcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def per_criterion_rates(results):\n",
        "    fields = PrefixRestoreAndContinuationEvaluation.model_fields.keys()\n",
        "    rates = {}\n",
        "\n",
        "    for field in fields:\n",
        "        rates[field] = sum(\n",
        "            getattr(r, field) for r in results\n",
        "        ) / len(results)\n",
        "\n",
        "    return rates\n",
        "\n",
        "\n",
        "print(\"Common:\")\n",
        "rates = per_criterion_rates(common_results)\n",
        "for k, v in rates.items():\n",
        "    print(f\"{k}: {v:.3f}\")\n",
        "\n",
        "print(\"Hybrid:\")\n",
        "rates = per_criterion_rates(hybrid_results)\n",
        "for k, v in rates.items():\n",
        "    print(f\"{k}: {v:.3f}\")"
      ],
      "metadata": {
        "id": "pPY48RHULY9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activations and attentions"
      ],
      "metadata": {
        "id": "Vy-407SwfN4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5CCGKOR6Ccm"
      },
      "outputs": [],
      "source": [
        "from compression_horizon.src.compression_horizon.inference.generation import calculate_outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Common\n",
        "compression_embeddings = load_compression_embeddings(\n",
        "    \"/content/drive/MyDrive/compression_horizon/common_loss/HuggingFaceTB/\"\n",
        "    \"SmolLM2-1.7B|128|1|bc818cdb-8346-4cf1-beb0-7459ce626638/compressed_prefixes/\",\n",
        "    device,\n",
        ")\n",
        "outputs = calculate_outputs(model, compression_embeddings, sequence_embeddings, attention_mask)"
      ],
      "metadata": {
        "id": "_WjKxbNLnKvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.attentions[0].shape  # torch.Size([1, 32, 129, 129])"
      ],
      "metadata": {
        "id": "pW-EQCwrTeTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(outputs.attentions)):\n",
        "    attn_layer = outputs.attentions[i]\n",
        "    attn_to_t = attn_layer[0, :, :, 0].mean(dim=0)\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.bar(range(len(attn_to_t)), attn_to_t.detach().cpu())\n",
        "    plt.title(f\"Attention paid TO compressed token, layer{i}\")\n",
        "    plt.xlabel(\"Query token index\")\n",
        "    plt.ylabel(\"Attention weight\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PpWTxfqiVBY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(5):\n",
        "    plt.plot(outputs[\"hidden_states\"][i].mean(dim=-1).squeeze(dim=0).cpu());"
      ],
      "metadata": {
        "id": "EPqRIc0ipVVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid\n",
        "compression_embeddings = load_compression_embeddings(\n",
        "    \"/content/drive/MyDrive/compression_horizon/17-11-2025/hybrid_loss/HuggingFaceTB/\"\n",
        "    \"SmolLM2-1.7B|128|1|0.01|cosine|1.0|5|978c86f1-57ac-4eba-a446-6a6dc874d451/compressed_prefixes/\",\n",
        "    device,\n",
        ")\n",
        "outputs = calculate_outputs(model, compression_embeddings, sequence_embeddings, attention_mask)"
      ],
      "metadata": {
        "id": "rfcHb35tnKsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b8owzeIFazDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(outputs.attentions)):\n",
        "    attn_layer = outputs.attentions[i]\n",
        "    attn_to_t = attn_layer[0, :, :, 0].mean(dim=0)\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.bar(range(len(attn_to_t)), attn_to_t.detach().cpu())\n",
        "    plt.title(f\"Attention paid TO compressed token, layer{i}\")\n",
        "    plt.xlabel(\"Query token index\")\n",
        "    plt.ylabel(\"Attention weight\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QiaJ0ocKaiG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(5):\n",
        "    plt.plot(outputs[\"hidden_states\"][i].mean(dim=-1).squeeze(dim=0).cpu());"
      ],
      "metadata": {
        "id": "7ajnjWhFpsjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F7ocPGt0Yhq"
      },
      "source": [
        "# Average distance between compression embeddings and (actual sequence/random sequence embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from compression_horizon.src.compression_horizon.metric import calculate_distances\n",
        "from compression_horizon.src.compression_horizon.inference.generation import calculate_outputs"
      ],
      "metadata": {
        "id": "Yl-tKEYYgsV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP9ylFKDtsSs"
      },
      "outputs": [],
      "source": [
        "actual_sequence  = input_ids\n",
        "actual_embeddings = sequence_embeddings\n",
        "\n",
        "random_sequence = torch.randint(0, tokenizer.vocab_size, input_ids.size(), device=device)\n",
        "with torch.no_grad():\n",
        "    random_embeddings = model.get_input_embeddings()(random_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nXgOmAn9J6W"
      },
      "outputs": [],
      "source": [
        "# Common\n",
        "compression_embeddings = load_compression_embeddings(\n",
        "    \"/content/drive/MyDrive/compression_horizon/common_loss/HuggingFaceTB/\"\n",
        "    \"SmolLM2-1.7B|128|1|bc818cdb-8346-4cf1-beb0-7459ce626638/compressed_prefixes/\",\n",
        "    device,\n",
        ")\n",
        "actual_outputs = calculate_outputs(model, compression_embeddings, sequence_embeddings, attention_mask)\n",
        "random_outputs = calculate_outputs(model, compression_embeddings, random_embeddings, attention_mask)\n",
        "\n",
        "for i in range(len(actual_outputs[\"hidden_states\"])):\n",
        "    print(f\"Hidden states layer {i}:\")\n",
        "    cosine, l2, l1 = calculate_distances(\n",
        "        actual_outputs[\"hidden_states\"][i][:, :1, :],\n",
        "        actual_outputs[\"hidden_states\"][i][:, 1:, :],\n",
        "    )\n",
        "    print(f\"Cosine: {cosine} | l2: {l2} | l1: {l1}\")\n",
        "    cosine, l2, l1 = calculate_distances(\n",
        "        random_outputs[\"hidden_states\"][i][:, :1, :],\n",
        "        random_outputs[\"hidden_states\"][i][:, 1:, :],\n",
        "    )\n",
        "    print(f\"Cosine: {cosine} | l2: {l2} | l1: {l1}\", end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid\n",
        "compression_embeddings = load_compression_embeddings(\n",
        "    \"/content/drive/MyDrive/compression_horizon/17-11-2025/hybrid_loss/HuggingFaceTB/\"\n",
        "    \"SmolLM2-1.7B|128|1|0.01|cosine|1.0|5|978c86f1-57ac-4eba-a446-6a6dc874d451/compressed_prefixes/\",\n",
        "    device,\n",
        ")\n",
        "actual_outputs = calculate_outputs(model, compression_embeddings, sequence_embeddings, attention_mask)\n",
        "random_outputs = calculate_outputs(model, compression_embeddings, random_embeddings, attention_mask)\n",
        "\n",
        "for i in range(len(actual_outputs[\"hidden_states\"])):\n",
        "    print(f\"Hidden states layer {i}:\")\n",
        "    cosine, l2, l1 = calculate_distances(\n",
        "        actual_outputs[\"hidden_states\"][i][:, :1, :],\n",
        "        actual_outputs[\"hidden_states\"][i][:, 1:, :],\n",
        "    )\n",
        "    print(f\"Cosine: {cosine} | l2: {l2} | l1: {l1}\")\n",
        "    cosine, l2, l1 = calculate_distances(\n",
        "        random_outputs[\"hidden_states\"][i][:, :1, :],\n",
        "        random_outputs[\"hidden_states\"][i][:, 1:, :],\n",
        "    )\n",
        "    print(f\"Cosine: {cosine} | l2: {l2} | l1: {l1}\", end=\"\\n\\n\")"
      ],
      "metadata": {
        "id": "sRWqQFZloEz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1HqEjbIJb0nF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "svtgDggbxwkr",
        "3c7yMlHkVevN",
        "24XuAv1iVYSJ",
        "tHjVlzgfjqlK",
        "Vy-407SwfN4a",
        "5F7ocPGt0Yhq"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}